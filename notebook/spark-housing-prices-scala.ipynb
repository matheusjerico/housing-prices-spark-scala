{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Housing Prices with Apache Spark (Scala) - ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Workflows\n",
    "1) ETL and analyzing historical data in order to extract the significant features and label; <br>\n",
    "2) Training, testing and evaluating the results of ML algorithms to build a model; <br>\n",
    "3) Using the model in production with new data to make predictions; <br>\n",
    "4) Model monitoring and model updating with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    ">These spatial data contain 20,640 observations on housing prices with 9 economic covariates. It appeared in Pace and Barry (1997), \"Sparse Spatial Autoregressions\", Statistics and Probability Letters. Submitted by Kelley Pace (kpace@unix1.sncc.lsu.edu). [9/Nov/99] (536 kbytes) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.104:4042\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1591566543549)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.ml._\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.regression._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.regression._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = ../dataset/housing.csv\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var file = \"../dataset/housing.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", true).load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|housing_median_age|       total_rooms|        population|median_house_value|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|             20640|             20640|             20640|             20640|\n",
      "|   mean|28.639486434108527|2635.7630813953488|1425.4767441860465|206855.81690891474|\n",
      "| stddev| 12.58555761211163|2181.6152515827944|  1132.46212176534|115395.61587441359|\n",
      "|    min|               1.0|               2.0|               3.0|           14999.0|\n",
      "|    max|              52.0|           39320.0|           35682.0|          500001.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"housing_median_age\",\"total_rooms\",\"population\",\"median_house_value\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n",
       "df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"rooms_per_house\", col(\"total_rooms\")/col(\"households\"))\n",
    "df = df.withColumn(\"pop_per_house\", col(\"population\")/col(\"households\"))\n",
    "df = df.withColumn(\"bedrooms_per_room\", col(\"total_bedrooms\")/col(\"total_rooms\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Drop tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(\"total_rooms\",\"households\", \"population\" , \"totalbedrooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+--------------+-------------+------------------+---------------+------------------+------------------+-------------------+\n",
      "|longitude|latitude|housing_median_age|total_bedrooms|median_income|median_house_value|ocean_proximity|   rooms_per_house|     pop_per_house|  bedrooms_per_room|\n",
      "+---------+--------+------------------+--------------+-------------+------------------+---------------+------------------+------------------+-------------------+\n",
      "|  -122.23|   37.88|              41.0|         129.0|       8.3252|          452600.0|       NEAR BAY| 6.984126984126984|2.5555555555555554|0.14659090909090908|\n",
      "|  -122.22|   37.86|              21.0|        1106.0|       8.3014|          358500.0|       NEAR BAY| 6.238137082601054| 2.109841827768014|0.15579659106916466|\n",
      "|  -122.24|   37.85|              52.0|         190.0|       7.2574|          352100.0|       NEAR BAY| 8.288135593220339|2.8022598870056497|0.12951601908657123|\n",
      "|  -122.25|   37.85|              52.0|         235.0|       5.6431|          341300.0|       NEAR BAY|5.8173515981735155| 2.547945205479452|0.18445839874411302|\n",
      "+---------+--------+------------------+--------------+-------------+------------------+---------------+------------------+------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. String to Index Ocean Proximity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_6fcfb93923a4\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer = new StringIndexer().setInputCol(\"ocean_proximity\")\n",
    "                                 .setOutputCol(\"ocean_proximity_in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 9 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var new_df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df=new_df.drop(\"ocean_proximity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+--------------+-------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_bedrooms|median_income|median_house_value|   rooms_per_house|     pop_per_house|  bedrooms_per_room|ocean_proximity_in|\n",
      "+---------+--------+------------------+--------------+-------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "|  -122.23|   37.88|              41.0|         129.0|       8.3252|          452600.0| 6.984126984126984|2.5555555555555554|0.14659090909090908|               3.0|\n",
      "|  -122.22|   37.86|              21.0|        1106.0|       8.3014|          358500.0| 6.238137082601054| 2.109841827768014|0.15579659106916466|               3.0|\n",
      "|  -122.24|   37.85|              52.0|         190.0|       7.2574|          352100.0| 8.288135593220339|2.8022598870056497|0.12951601908657123|               3.0|\n",
      "|  -122.25|   37.85|              52.0|         235.0|       5.6431|          341300.0|5.8173515981735155| 2.547945205479452|0.18445839874411302|               3.0|\n",
      "+---------+--------+------------------+--------------+-------------+------------------+------------------+------------------+-------------------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.cache()\n",
    "new_df.createOrReplaceTempView(\"house\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|corr(median_house_value, median_income)|\n",
      "+---------------------------------------+\n",
      "|                     0.6880752079585578|\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(corr(\"median_house_value\", \"median_income\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|corr(median_house_value, bedrooms_per_room)|\n",
      "+-------------------------------------------+\n",
      "|                       -0.25588014941949866|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(corr(\"median_house_value\", \"bedrooms_per_room\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|corr(median_house_value, pop_per_house)|\n",
      "+---------------------------------------+\n",
      "|                   -0.02373741295613...|\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(corr(\"median_house_value\", \"pop_per_house\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|corr(median_house_value, ocean_proximity_in)|\n",
      "+--------------------------------------------+\n",
      "|                        0.021732204251456527|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(corr(\"median_house_value\", \"ocean_proximity_in\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [longitude: double, latitude: double ... 8 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [longitude: double, latitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainingData, testData) = new_df.randomSplit(Array(0.8, 0.2), 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Extraction and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[String] = Array(longitude, latitude, housing_median_age, total_bedrooms, median_income, median_house_value, rooms_per_house, pop_per_house, bedrooms_per_room, ocean_proximity_in)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Features columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCols: Array[String] = Array(housing_median_age, median_income, rooms_per_house, pop_per_house, bedrooms_per_room, longitude, latitude, ocean_proximity_in)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCols = Array(\"housing_median_age\", \"median_income\", \"rooms_per_house\", \"pop_per_house\",\n",
    "                        \"bedrooms_per_room\", \"longitude\", \"latitude\", \"ocean_proximity_in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Create VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_73f60601072f\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setHandleInvalid(\"skip\")\n",
    "                                     .setInputCols(featureCols)\n",
    "                                     .setOutputCol(\"rawfeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3. Create a transformer StandardScaler to standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_697c75bec51a\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler().setInputCol(\"rawfeatures\")\n",
    "                                 .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4. Model Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_b5c5b43f6af0\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestRegressor().setLabelCol(\"median_house_value\")\n",
    "                                    .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "steps: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(vecAssembler_73f60601072f, stdScal_697c75bec51a, rfr_b5c5b43f6af0)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val steps = Array(assembler, scaler, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_ecd2f534f225\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline().setStages(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train the model \n",
    "- cross validation\n",
    "- tunning hiperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfr_b5c5b43f6af0-maxBins: 50,\n",
       "\trfr_b5c5b43f6af0-maxDepth: 7,\n",
       "\trfr_b5c5b43f6af0-numTrees: 20\n",
       "}, {\n",
       "\trfr_b5c5b43f6af0-maxBins: 50,\n",
       "\trfr_b5c5b43f6af0-maxDepth: 7,\n",
       "\trfr_b5c5b43f6af0-numTrees: 40\n",
       "}, {\n",
       "\trfr_b5c5b43f6af0-maxBins: 50,\n",
       "\trfr_b5c5b43f6af0-maxDepth: 10,\n",
       "\trfr_b5c5b43f6af0-numTrees: 20\n",
       "}, {\n",
       "\trfr_b5c5b43f6af0-maxBins: 50,\n",
       "\trfr_b5c5b43f6af0-maxDepth: 10,\n",
       "\trfr_b5c5b43f6af0-numTrees: 40\n",
       "}, {\n",
       "\trfr_b5c5b43f6af0-maxBins: 50,\n",
       "\trfr_b5c5b43f6af0-maxDepth: 20,\n",
       "\trfr_b5c5b43f6af0-numTrees: 20\n",
       "}, {\n",
       "\trfr_b5c5b43f6af0-maxBins: 50,\n",
       "\trfr_b5c5b43f6af0-maxDepth: 20,\n",
       "\trfr_b5c5b43f6af0-numTrees: 40\n",
       "}, {\n",
       "\trfr_b5c5b43f6af0-maxBins: 100,\n",
       "\trfr_b5c5b43f6af0-maxDepth: 7,\n",
       "\trfr_b5c5b43f6af0-numTrees: 20\n",
       "}, {\n",
       "\trfr_b5c5b43f6af0-maxBins: 100,\n",
       "\trfr_b5c5b4..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramGrid = new ParamGridBuilder().addGrid(rf.maxBins, Array(50, 100))\n",
    "                                      .addGrid(rf.maxDepth, Array(7, 10, 20))\n",
    "                                      .addGrid(rf.numTrees, Array(20, 40))\n",
    "                                      .build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_fc5902f1f3d6\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new RegressionEvaluator().setLabelCol(\"median_house_value\")\n",
    "                                         .setPredictionCol(\"prediction\")\n",
    "                                         .setMetricName(\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossvalidator: org.apache.spark.ml.tuning.CrossValidator = cv_919a457a47f7\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossvalidator = new CrossValidator().setEstimator(pipeline)\n",
    "                                         .setEvaluator(evaluator)\n",
    "                                         .setEstimatorParamMaps(paramGrid)\n",
    "                                         .setNumFolds(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_919a457a47f7\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineModel = crossvalidator.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: median_income, importance: 0.3632059158120435\n",
      "feature: pop_per_house, importance: 0.11961049174330007\n",
      "feature: longitude, importance: 0.10312807196865144\n",
      "feature: bedrooms_per_room, importance: 0.0987422354638934\n",
      "feature: latitude, importance: 0.09767896347515746\n",
      "feature: ocean_proximity_in, importance: 0.08726479259333791\n",
      "feature: rooms_per_house, importance: 0.06824258298883883\n",
      "feature: housing_median_age, importance: 0.06212694595477756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featureImportances: org.apache.spark.ml.linalg.Vector = (8,[0,1,2,3,4,5,6,7],[0.06212694595477756,0.3632059158120435,0.06824258298883883,0.11961049174330007,0.0987422354638934,0.10312807196865144,0.09767896347515746,0.08726479259333791])\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureImportances = pipelineModel.bestModel\n",
    "                                      .asInstanceOf[PipelineModel]\n",
    "                                      .stages(2)\n",
    "                                      .asInstanceOf[RandomForestRegressionModel]\n",
    "                                      .featureImportances\n",
    "\n",
    "assembler.getInputCols.zip(featureImportances.toArray)\n",
    "                      .sortBy(-_._2)\n",
    "                      .foreach { case (feat, imp) => println(s\"feature: $feat, importance: $imp\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "{\n",
      "\trfr_b5c5b43f6af0-maxBins: 50,\n",
      "\trfr_b5c5b43f6af0-maxDepth: 7,\n",
      "\trfr_b5c5b43f6af0-numTrees: 40\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bestEstimatorParamMap: org.apache.spark.ml.param.ParamMap =\n",
       "{\n",
       "\trfr_b5c5b43f6af0-maxBins: 50,\n",
       "\trfr_b5c5b43f6af0-maxDepth: 7,\n",
       "\trfr_b5c5b43f6af0-numTrees: 40\n",
       "}\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bestEstimatorParamMap = pipelineModel.getEstimatorParamMaps\n",
    "                                         .zip(pipelineModel.avgMetrics)\n",
    "                                         .maxBy(_._2)\n",
    "                                         ._1\n",
    "println(s\"Best params:\\n$bestEstimatorParamMap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Prediction and model evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 11 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = pipelineModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|       prediction|median_house_value|\n",
      "+-----------------+------------------+\n",
      "|         126910.0|           94600.0|\n",
      "|         78556.25|           85800.0|\n",
      "|97337.77777777778|          103600.0|\n",
      "|112033.6111111111|           90100.0|\n",
      "|85403.92857142857|           82800.0|\n",
      "+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\", \"median_house_value\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+\n",
      "|       prediction|median_house_value|             error|\n",
      "+-----------------+------------------+------------------+\n",
      "|         126910.0|           94600.0|           32310.0|\n",
      "|         78556.25|           85800.0|          -7243.75|\n",
      "|97337.77777777778|          103600.0|-6262.222222222219|\n",
      "|112033.6111111111|           90100.0|21933.611111111095|\n",
      "|85403.92857142857|           82800.0| 2603.928571428565|\n",
      "|80827.41239316239|           81300.0|-472.5876068376092|\n",
      "|76196.41666666666|           62500.0|13696.416666666657|\n",
      "|         128485.0|          109400.0|           19085.0|\n",
      "|83765.16666666666|           76900.0| 6865.166666666657|\n",
      "|         87844.75|           74100.0|          13744.75|\n",
      "|97743.91666666666|           80500.0|17243.916666666657|\n",
      "|97232.46323529413|           66800.0|30432.463235294126|\n",
      "|       117778.125|           74100.0|         43678.125|\n",
      "|         134427.5|           55000.0|           79427.5|\n",
      "|         131047.5|          111800.0|           19247.5|\n",
      "|106078.6388888889|           92600.0|13478.638888888905|\n",
      "|87882.21323529413|           74100.0|13782.213235294126|\n",
      "|          83853.5|          105800.0|          -21946.5|\n",
      "|       108567.475|           78800.0|29767.475000000006|\n",
      "|       130183.125|          106300.0|         23883.125|\n",
      "+-----------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_error: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 12 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_error = predictions.withColumn(\"error\", col(\"prediction\")-col(\"median_house_value\"))\n",
    "predictions_error.select(\"prediction\", \"median_house_value\", \"error\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|        prediction|median_house_value|             error|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              4127|              4127|              4127|\n",
      "|   mean|206842.43086360718|205955.60649382116| 886.8243697859892|\n",
      "| stddev|102159.15633129653| 115007.9470486048| 46584.00835777262|\n",
      "|    min|           48767.5|           26900.0|-331935.5416666667|\n",
      "|    max|          500001.0|          500001.0|         294377.71|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_error.describe(\"prediction\", \"median_house_value\", \"error\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_46f9c13b9c67\n",
       "mae: Double = 30740.695947490512\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maevaluator = new RegressionEvaluator().setLabelCol(\"median_house_value\")\n",
    "                                           .setMetricName(\"mae\")\n",
    "val mae = maevaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_3b8c63a30d26\n",
       "rmse: Double = 46586.80574528031\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new RegressionEvaluator().setLabelCol(\"median_house_value\")\n",
    "                                         .setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineModel.write.overwrite().save(\"modeldir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sameModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_919a457a47f7\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sameModel = CrossValidatorModel.load(\"modeldir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
